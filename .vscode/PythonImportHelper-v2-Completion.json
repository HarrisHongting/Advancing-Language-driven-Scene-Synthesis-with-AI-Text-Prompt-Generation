[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "posa.data_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "posa.data_utils",
        "description": "posa.data_utils",
        "detail": "posa.data_utils",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "trimesh",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "trimesh",
        "description": "trimesh",
        "detail": "trimesh",
        "documentation": {}
    },
    {
        "label": "open3d",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open3d",
        "description": "open3d",
        "detail": "open3d",
        "documentation": {}
    },
    {
        "label": "chamfer_distance",
        "importPath": "pytorch3d.loss",
        "description": "pytorch3d.loss",
        "isExtraImport": true,
        "detail": "pytorch3d.loss",
        "documentation": {}
    },
    {
        "label": "chamfer_distance",
        "importPath": "pytorch3d.loss",
        "description": "pytorch3d.loss",
        "isExtraImport": true,
        "detail": "pytorch3d.loss",
        "documentation": {}
    },
    {
        "label": "randrange",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "sklearn.cluster",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "posa.vis_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "posa.vis_utils",
        "description": "posa.vis_utils",
        "detail": "posa.vis_utils",
        "documentation": {}
    },
    {
        "label": "posa.general_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "posa.general_utils",
        "description": "posa.general_utils",
        "detail": "posa.general_utils",
        "documentation": {}
    },
    {
        "label": "compute_recon_loss",
        "importPath": "posa.general_utils",
        "description": "posa.general_utils",
        "isExtraImport": true,
        "detail": "posa.general_utils",
        "documentation": {}
    },
    {
        "label": "compute_delta",
        "importPath": "posa.general_utils",
        "description": "posa.general_utils",
        "isExtraImport": true,
        "detail": "posa.general_utils",
        "documentation": {}
    },
    {
        "label": "ProxDataset_txt",
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "isExtraImport": true,
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "HUMANISE",
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "isExtraImport": true,
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "ProxDataset_txt",
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "isExtraImport": true,
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "HUMANISE",
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "isExtraImport": true,
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "create_model_and_diffusion",
        "importPath": "util.model_util",
        "description": "util.model_util",
        "isExtraImport": true,
        "detail": "util.model_util",
        "documentation": {}
    },
    {
        "label": "create_model_and_diffusion",
        "importPath": "util.model_util",
        "description": "util.model_util",
        "isExtraImport": true,
        "detail": "util.model_util",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "util.evaluation",
        "description": "util.evaluation",
        "isExtraImport": true,
        "detail": "util.evaluation",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "count_parameters",
        "importPath": "posa.posa_utils",
        "description": "posa.posa_utils",
        "isExtraImport": true,
        "detail": "posa.posa_utils",
        "documentation": {}
    },
    {
        "label": "create_named_schedule_sampler",
        "importPath": "diffusion.resample",
        "description": "diffusion.resample",
        "isExtraImport": true,
        "detail": "diffusion.resample",
        "documentation": {}
    },
    {
        "label": "MixedPrecisionTrainer",
        "importPath": "diffusion.fp16_util",
        "description": "diffusion.fp16_util",
        "isExtraImport": true,
        "detail": "diffusion.fp16_util",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "ProxSegDataset",
        "kind": 6,
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "peekOfCode": "class ProxSegDataset(Dataset):\n    def __init__(self, data_dir, fix_orientation=False, no_obj_classes=8, train_seg_len=32,\n                 ds_weights_path=\"posa/support_files/downsampled_weights.npy\", jump_step=1, step_multiplier=1, **kwargs):\n        self.data_dir = data_dir\n        self.semantics_dir = os.path.join(data_dir, \"semantics\")\n        self.vertices_can_dir = os.path.join(data_dir, \"vertices_can\")\n        self.seq_names = [f.split('cf')[0] for f in os.listdir(self.semantics_dir)]\n        self.vertices_can_dict = dict()\n        self.semantics_dict = dict()\n        self.total_frames = 0",
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "ProxSegDataset_seq",
        "kind": 6,
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "peekOfCode": "class ProxSegDataset_seq(Dataset):\n    def __init__(self, data_dir, fix_orientation=False, no_obj_classes=8, train_seg_len=32, num_seg=8,\n                 ds_weights_path=\"posa/support_files/downsampled_weights.npy\", step_multiplier=1, stride=32,\n                 jump_step=1, **kwargs):\n        self.data_dir = data_dir\n        self.contacts_s_dir = os.path.join(data_dir, \"semantics\")\n        self.vertices_can_dir = os.path.join(data_dir, \"vertices_can\")\n        self.seq_names = [f.split('cfs')[0] for f in os.listdir(self.contacts_s_dir)]\n        self.vertices_can_dict = dict()\n        self.contacts_s_dict = dict()",
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "ProxSegDataset_var",
        "kind": 6,
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "peekOfCode": "class ProxSegDataset_var(Dataset):    # when jump_step=8, for a whole seq, dataset's max_frame is 165, max num_seg is 29\n    def __init__(self, data_dir, fix_orientation=False, no_obj_classes=8, max_frame=128, num_seg=10, dist_eps=0.7,\n                 ds_weights_path=\"posa/support_files/downsampled_weights.npy\", jump_step=8, step_multiplier=1, **kwargs):\n        self.data_dir = data_dir\n        self.contacts_s_dir = os.path.join(data_dir, \"semantics\")\n        self.vertices_can_dir = os.path.join(data_dir, \"vertices_can\")\n        self.vertices_dir = os.path.join(data_dir, \"vertices\")\n        self.seq_names = [f.split('cfs')[0] for f in os.listdir(self.contacts_s_dir)]\n        self.vertices_can_dict = dict()\n        self.vertices_dict = dict()",
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "ProxDataset_ds",
        "kind": 6,
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "peekOfCode": "class ProxDataset_ds(Dataset):    # when jump_step=8, for a whole seq, dataset's max_frame is 165, max num_seg is 29\n    def __init__(self, data_dir, fix_orientation=False, no_obj_classes=8, max_frame=220,\n                 ds_weights_path=\"posa/support_files/downsampled_weights.npy\", jump_step=8, step_multiplier=1, **kwargs):\n        '''\n            data_dir: directory that stores processed PROXD dataset.\n            fix_orientation: flag that specifies whether we always make the first pose in a motion sequence facing\n                             towards a canonical direction.\n            no_obj_classes: number of contact object classes.\n            max_frame: the maximum motion sequence length which the model accepts (after applying frame skipping).\n            ds_weights_path: the saved downsampling matrix for downsampling body vertices.",
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "ProxDataset_txt",
        "kind": 6,
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "peekOfCode": "class ProxDataset_txt(Dataset):    # when jump_step=8, for a whole seq, dataset's max_frame is 165, max num_seg is 29\n    def __init__(self, data_dir, batch_num =1, fix_orientation=False, no_obj_classes=8, max_frame=220,\n                 ds_weights_path=\"posa/support_files/downsampled_weights.npy\", jump_step=8, step_multiplier=1, max_objs=8, pnt_size=1024, \n                 objs_data_dir='data/protext/objs', max_cats=13, **kwargs):\n        '''\n            data_dir: directory that stores processed PROXD dataset.\n            fix_orientation: flag that specifies whether we always make the first pose in a motion sequence facing\n                             towards a canonical direction.\n            no_obj_classes: number of contact object classes.\n            max_frame: the maximum motion sequence length which the model accepts (after applying frame skipping).",
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "HUMANISE",
        "kind": 6,
        "importPath": "posa.dataset",
        "description": "posa.dataset",
        "peekOfCode": "class HUMANISE(Dataset):    # when jump_step=8, for a whole seq, dataset's max_frame is 165, max num_seg is 29\n    def __init__(self, data_dir, fix_orientation=False, no_obj_classes=8, max_frame=220,\n                 ds_weights_path=\"posa/support_files/downsampled_weights.npy\", jump_step=8, step_multiplier=1, max_objs=8, pnt_size=1024, \n                 objs_data_dir='data/humanise/objs', max_cats=11, **kwargs):\n        '''\n            data_dir: directory that stores processed PROXD dataset.\n            fix_orientation: flag that specifies whether we always make the first pose in a motion sequence facing\n                             towards a canonical direction.\n            no_obj_classes: number of contact object classes.\n            max_frame: the maximum motion sequence length which the model accepts (after applying frame skipping).",
        "detail": "posa.dataset",
        "documentation": {}
    },
    {
        "label": "list_mean",
        "kind": 2,
        "importPath": "run.test_sdm",
        "description": "run.test_sdm",
        "peekOfCode": "def list_mean(list):\n    acc = 0.\n    for item in list:\n        acc += item\n    return acc / len(list)\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\"data_dir\", type=str,\n                        help=\"path to POSA_temp dataset dir\")\n    parser.add_argument(\"--load_model\", type=str, default=\"training/contactformer/model_ckpt/best_model_recon_acc.pt\",",
        "detail": "run.test_sdm",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "run.train_sdm",
        "description": "run.train_sdm",
        "peekOfCode": "def train():\n    # Create diffusion sampler, optimizer, and trainer\n    schedule_sampler_type = 'uniform'\n    schedule_sampler = create_named_schedule_sampler(schedule_sampler_type, diffusion)\n    use_fp16 = False\n    fp16_scale_growth = 1e-3\n    mp_trainer = MixedPrecisionTrainer(\n            model=model,\n            use_fp16=use_fp16,\n            fp16_scale_growth=fp16_scale_growth,",
        "detail": "run.train_sdm",
        "documentation": {}
    },
    {
        "label": "validate",
        "kind": 2,
        "importPath": "run.train_sdm",
        "description": "run.train_sdm",
        "peekOfCode": "def validate():\n    use_ddim = False  # FIXME - hardcoded\n    clip_denoised = False  # FIXME - hardcoded\n    model.eval()\n    with torch.no_grad():\n        sample_fn = (\n            diffusion.p_sample_loop if not use_ddim else diffusion.ddim_sample_loop\n        )\n        total_recon_loss_semantics = 0\n        total_cfd = 0",
        "detail": "run.train_sdm",
        "documentation": {}
    },
    {
        "label": "log_directory",
        "kind": 5,
        "importPath": "run.train_sdm",
        "description": "run.train_sdm",
        "peekOfCode": "log_directory = \"logs\"\nif not os.path.exists(log_directory):\n    os.makedirs(log_directory) \nlog_file_path = os.path.join(log_directory, \"training_log.log\")\nlogging.basicConfig(filename=log_file_path, level=logging.INFO, filemode='w', format='%(levelname)s:%(message)s')\ndef train():\n    # Create diffusion sampler, optimizer, and trainer\n    schedule_sampler_type = 'uniform'\n    schedule_sampler = create_named_schedule_sampler(schedule_sampler_type, diffusion)\n    use_fp16 = False",
        "detail": "run.train_sdm",
        "documentation": {}
    },
    {
        "label": "log_file_path",
        "kind": 5,
        "importPath": "run.train_sdm",
        "description": "run.train_sdm",
        "peekOfCode": "log_file_path = os.path.join(log_directory, \"training_log.log\")\nlogging.basicConfig(filename=log_file_path, level=logging.INFO, filemode='w', format='%(levelname)s:%(message)s')\ndef train():\n    # Create diffusion sampler, optimizer, and trainer\n    schedule_sampler_type = 'uniform'\n    schedule_sampler = create_named_schedule_sampler(schedule_sampler_type, diffusion)\n    use_fp16 = False\n    fp16_scale_growth = 1e-3\n    mp_trainer = MixedPrecisionTrainer(\n            model=model,",
        "detail": "run.train_sdm",
        "documentation": {}
    },
    {
        "label": "check_file_lines",
        "kind": 2,
        "importPath": "check",
        "description": "check",
        "peekOfCode": "def check_file_lines(directory):\n    for filename in os.listdir(directory):\n        if filename.endswith(\".txt\"): \n            filepath = os.path.join(directory, filename)\n            with open(filepath, 'r') as file:\n                lines = file.readlines()\n                if len(lines) != 3:\n                    print(f\"File {filename} has {len(lines)} lines.\")\ndirectory_path = 'data/protext/proxd_train/context_versions/batch_10'  # REPLACE PATH\ncheck_file_lines(directory_path)",
        "detail": "check",
        "documentation": {}
    },
    {
        "label": "directory_path",
        "kind": 5,
        "importPath": "check",
        "description": "check",
        "peekOfCode": "directory_path = 'data/protext/proxd_train/context_versions/batch_10'  # REPLACE PATH\ncheck_file_lines(directory_path)",
        "detail": "check",
        "documentation": {}
    },
    {
        "label": "generate_prompts_and_save",
        "kind": 2,
        "importPath": "groq_prompts",
        "description": "groq_prompts",
        "peekOfCode": "def generate_prompts_and_save(source_dir, target_dir_base):\n    if not os.path.exists(target_dir_base):\n        os.makedirs(target_dir_base)\n    files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]\n    temperatures = [0.5 + 0.05 * i for i in range(10)] \n    request_count = 0\n    for index, temp in enumerate(temperatures):\n        target_dir = os.path.join(target_dir_base, f'batch_{index + 1}')\n        os.makedirs(target_dir, exist_ok=True) \n        for filename in files:",
        "detail": "groq_prompts",
        "documentation": {}
    },
    {
        "label": "generate_text_with_api",
        "kind": 2,
        "importPath": "groq_prompts",
        "description": "groq_prompts",
        "peekOfCode": "def generate_text_with_api(text, temperature):\n    headers = {\n        \"Authorization\": f\"Bearer {API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n    data = {\n        \"model\": \"llama3-8b-8192\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant. What I am sending to you now is the text prompt of my project. These text instructions will be converted into embedding vectors and combined with my other point cloud data to generate the scene corresponding to the text prompt. Now I need you to help me overwrite this text prompt to improve the accuracy of the model. No extra words are needed, just ONLY provide the rewritten language.\"},\n            {\"role\": \"user\", \"content\": text}",
        "detail": "groq_prompts",
        "documentation": {}
    },
    {
        "label": "API_KEY",
        "kind": 5,
        "importPath": "groq_prompts",
        "description": "groq_prompts",
        "peekOfCode": "API_KEY = 'YOUR_API_KEY'\nAPI_URL = 'https://api.groq.com/openai/v1/chat/completions'\ndef generate_prompts_and_save(source_dir, target_dir_base):\n    if not os.path.exists(target_dir_base):\n        os.makedirs(target_dir_base)\n    files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]\n    temperatures = [0.5 + 0.05 * i for i in range(10)] \n    request_count = 0\n    for index, temp in enumerate(temperatures):\n        target_dir = os.path.join(target_dir_base, f'batch_{index + 1}')",
        "detail": "groq_prompts",
        "documentation": {}
    },
    {
        "label": "API_URL",
        "kind": 5,
        "importPath": "groq_prompts",
        "description": "groq_prompts",
        "peekOfCode": "API_URL = 'https://api.groq.com/openai/v1/chat/completions'\ndef generate_prompts_and_save(source_dir, target_dir_base):\n    if not os.path.exists(target_dir_base):\n        os.makedirs(target_dir_base)\n    files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]\n    temperatures = [0.5 + 0.05 * i for i in range(10)] \n    request_count = 0\n    for index, temp in enumerate(temperatures):\n        target_dir = os.path.join(target_dir_base, f'batch_{index + 1}')\n        os.makedirs(target_dir, exist_ok=True) ",
        "detail": "groq_prompts",
        "documentation": {}
    },
    {
        "label": "source_directory",
        "kind": 5,
        "importPath": "groq_prompts",
        "description": "groq_prompts",
        "peekOfCode": "source_directory = 'data/protext/proxd_train/context'\ntarget_directory_base = 'data/protext/proxd_train/context_versions'\ngenerate_prompts_and_save(source_directory, target_directory_base)",
        "detail": "groq_prompts",
        "documentation": {}
    },
    {
        "label": "target_directory_base",
        "kind": 5,
        "importPath": "groq_prompts",
        "description": "groq_prompts",
        "peekOfCode": "target_directory_base = 'data/protext/proxd_train/context_versions'\ngenerate_prompts_and_save(source_directory, target_directory_base)",
        "detail": "groq_prompts",
        "documentation": {}
    }
]